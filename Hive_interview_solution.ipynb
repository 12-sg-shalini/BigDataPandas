{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a2f187-95fb-463f-a360-b18349452a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###1.What is the definition of Hive? What is the present version of Hive?\n",
    "Ans:- Hive is a data warehouse open-source framework built on the top of Hadoop. It is used t - analyze structured data.\n",
    "It provides a SQL like interface(HQL) t - query data stored in various databases and file systems. Hive 3.13 is the \n",
    "latest and stable version of hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba67163-8ee0-4dcc-b1ac-865da4ac6c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###2.Is Hive suitable to be used for OLTP systems? Why?\n",
    "Ans:- No, hive is not suitable t - be used for OLTP system, because hive is not made t - be used for frequent update and\n",
    "delete purpose. Hive framework is made t - store once and read many times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462d16b7-5080-4784-9b24-10ca286498bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###3.How is HIVE different from RDBMS? Does hive support ACID transactions. If not then give the proper reason.\n",
    "Ans:- Hive supports OLAP system while the RDBMS is an OLTP system. Hive is made t - write once and read many times.\n",
    "RDBMS deals with fixed schema while hive supports both fixed and dynamic schema. Acid transactions are not supported in \n",
    "hive because hive is used for OLAP system and only support insert/Delete command but not the update command.\n",
    "However, from hive version 0.14, we can create a transactional table that stores the ORC file format and this will\n",
    "support the ACID transaction. (we need t - manually enable the acid property)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe85d82-eb46-4b9d-be27-898b8dc79a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###4.Explain the hive architecture and the different components of a Hive architecture?\n",
    "Ans:-\n",
    "Hive client:- Hive allows writing applications in various languages(Java, Python, Scala). It supports different types of clients such as:-Trift server, JDBC Driver and ODBC Driver. This enables all the requests from various clients\n",
    "\n",
    "Hive Services:-\n",
    "1. Hive CLI/web UI - The Hive CLI (Command Line Interface) is a shell where we can execute Hive queries and commands. Hive web UI is a web UI similar t - hive CLI. We can als - use 3rd party platform like putty in place of CLI.\n",
    "2. Hive Metastore:- it stores the the information of location, metadata, partitions, serialization and deserialization for various tables in warehouse.\n",
    "3. Apache server:- It is referred t - as Apache Thrift Server. It accepts the request from different clients and provides it t - Hive Driver.\n",
    "4. Hive Driver - It receives queries from different sources like web UI, CLI, Thrift, and JDBC/ODBC driver. It transfers the queries t - the compiler\n",
    "5. Hive Compiler - It performs the query and sementic checks, It converts HiveQL statements int - MapReduce jobs.\n",
    "6. Hive Execution Engine- It generates the logical plan and execute it in the form of DAG of map-reduce tasks and HDFS tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eb88c8-2357-4988-ae32-0b2ad543892b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "###5.Mention what Hive query processor does? And Mention what are the components of a Hive query processor?\n",
    "This component implements the processing framework for converting SQL to a graph of map/reduce jobs and the execution \n",
    "time framework to run those jobs in the order of dependencies\n",
    "Following are the components of a Hive Query Processor:\n",
    "Parse and Semantic Analysis (ql/parse)\n",
    "Metadata Layer (ql/metadata)\n",
    "Type Interfaces (ql/typeinfo)\n",
    "Sessions (ql/session)\n",
    "Map/Reduce Execution Engine (ql/exec)\n",
    "Plan Components (ql/plan)\n",
    "Hive Function Framework (ql/udf)\n",
    "Tools (ql/tools)\n",
    "Optimizer (ql/optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f8487-a7fd-4ddc-a642-5e42d69da1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "###6.What are the three different modes in which we can operate Hive?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a0bf57-82de-466e-a9df-e113d14dbdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###7.Features and Limitations of Hive.\n",
    "Ans:- \n",
    "Features:-\n",
    "1. Open Source\n",
    "2. Support HQL\n",
    "3. Support multiple users query execution\n",
    "4. Partitioning and Bucketing\n",
    "5. Support File format like:- ORC, Avro, Parquet etc.\n",
    "6. Support Olap warehouse,\n",
    "7. Support HDFS.\n",
    "\n",
    "Limitation:-\n",
    "1. Does not support OLTP.\n",
    "2. Don’t support real time queries\n",
    "3. Limited subqueries support\n",
    "4. Hive execution is slow and with high latency,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03437fc3-7ea5-48c2-ac49-81efe0047ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###8.How to create a Database in HIVE?\n",
    "Ans:- enter t - hive cli/shell and write the command:- create database if not exists database_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4048a3c9-bbf3-48ea-b9f2-9a503cf8c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###9.How to create a table in HIVE?\n",
    "Ans:-\n",
    "Internal Table --\n",
    "create table table_name (col1 dataype, col_2 datatype) row format delimited feilds terminated by ',';\n",
    "\n",
    "External Table --\n",
    "create external table table_name (col1 dataype, col_2 datatype) row format delimited feilds terminated by ',' location 'location_of_files_in_hdfs';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6c8a13-890e-4d19-81ce-9979553d5f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###10.What do you mean by describe and describe extended and describe formatted with respect to database and table?\n",
    "Ans:-\n",
    "Describe Formatted -- It displays whether table is internal or external, when it was created, the file format, the location of the data in HDFS, FORMATTED is specified, it show/displays the metadata in a tabular format.\n",
    "Describe -- The DESCRIBE output for a database includes the location and the comment. als - provides the column and datatype for column details for tables\n",
    "Extended -- If the EXTENDED is specified, it show/displays all the metadata for the specified table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1dab73-87ca-45cb-9b0d-26e30bcdb37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###11.How to skip header rows from a table in Hive?\n",
    "we can use these properties tblproperties (\"skip.header.line.count\"=\"1\")and header will skip in the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175e5337-ecc1-47e4-ab4b-2b17e5945c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "###12.What is a hive operator? What are the different types of hive operators?\n",
    "There are four types of operators in Hive:\n",
    "Relational Operators\n",
    "Arithmetic Operators\n",
    "Logical Operators\n",
    "Complex Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a82428-5a2b-4bee-8618-727d3acbbd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###13.Explain about the Hive Built-In Functions\n",
    "Ans:- T - perform some specific mathematical and arithmetic operations, Hive have some built-in functions. These built-in functions extract data from tables in hive and process the calculations.\n",
    "Eg:- Mathematical function, Collection function, Type conversion function, Date function, Conditional function, and String function.\n",
    "T - check list of built-in functions, use this command -- show functions;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0cd63f-f0bf-43ae-9e02-3aa8261a540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###14. Write hive DDL and DML commands.\n",
    "Examples of DDL commands: CREATE TABLE, ALTER TABLE, DROP TABLE, TRUNCATE TABLE, and RENAME TABLE.\n",
    "Examples of DML commands: SELECT, INSERT, UPDATE, DELETE, and MERGE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec1e37e-3761-4f24-9986-7afb857a4a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###15.Explain about SORT BY, ORDER BY, DISTRIBUTE BY and CLUSTER BY in Hive.\n",
    "sort by:- It can use in multiple reducers for final output. It orders the data per reducer locally.\n",
    "Order by:- Uses single reducer t - guarantee total order in output. ie. Even if we increase the reducer it will only use a single reducer. It sort the data per a single reducer globally. LIMIT can be used t - minimize sort time.\n",
    "Distribute by:- It distribute the input rows among reducers. It ensures that all rows for the same key columns are going t - the same reducer. It does not sort the data globally. N - overlapping of data.\n",
    "Cluster by:- CLUSTER BY clause is equivalent t - the output of DISTRIBUTE BY + SORT BY clauses. The CLUSTER BY clause distributes the data based on the key column and then sorts the output data by putting the same key column values adjacent t - each other. The output of the CLUSTER BY clause is sorted at the reducer level(Locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f95e860-c9c9-43c6-9aee-af408af396da",
   "metadata": {},
   "outputs": [],
   "source": [
    "###16.Difference between \"Internal Table\" and \"External Table\" and Mention when to choose “Internal Table” and “External Table” in Hive?\n",
    "Ans:- \n",
    "Internal Table:-\n",
    " - It is a default table in hive.\n",
    " - Whenenver we create a hive table without mentioning the keyword 'external', a internal(managed) table gets created.\n",
    " - The default location of the internal table is /user/hive/warehouse\n",
    " - We can change the default location of the internal table by specifying int - location keyword.\n",
    " - If we delete the internal table data and the metadata associated with that table will be deleted from the HDFS.\n",
    "\n",
    "External Table:-\n",
    " - Hive does not manage the data of the External table.\n",
    " - We create an external table for external use as when we want to use the data outside the Hive.\n",
    " - We can create the external table by specifying the EXTERNAL keyword.\n",
    " - They can access data stored in sources such as remote HDFS locations.\n",
    " - Whenever we drop the external table, then only the metadata associated with the table will get deleted, the table data remains untouched by Hive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3076719-1bf9-4f8d-ad07-bbed5ab574be",
   "metadata": {},
   "outputs": [],
   "source": [
    "###17.Where does the data of a Hive table get stored?\n",
    "Ans:- /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188003d7-aea4-48e6-96d0-17daa8441c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    " ###18.Is it possible to change the default location of a managed table?\n",
    "Yes, you can do it by using the clause – LOCATION ‘<hdfs_path>’ we can change the default location of a managed table.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c1a4ba-75ca-46ee-9207-d21a217f1f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###19.What is a metastore in Hive? What is the default database provided by Apache Hive for metastore?\n",
    "Ans:- Metastore is nothing but a traditional RDBMS in the hive. When we create a new Hive table, the information related to the schema (column names, data types, location) is stored in the Hive metastore relational database. Derby is a default metastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb8e94d-f47d-4ab9-9ee4-9918cadeea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "###20.Why does Hive not store metadata information in HDFS?\n",
    "Ans:- To achieve high performance, speed and low latency hive uses metastore. HDFS read/write operations are time consuming processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a7eb7-ba19-47c8-aaad-522149f82b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "###21.What is a partition in Hive? And Why do we perform partitioning in Hive?\n",
    "Ans:- Dividing the entire big table into some parts based on the values of a single or multiple columns like date, course, city or country.\n",
    "To restrict the hive to scan the entire table. while scan the particular partition increase the query performance time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcfe056-cbed-4143-a095-3325ced6d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###22.What is the difference between dynamic partitioning and static partitioning?\n",
    "Ans:- \n",
    "Static Partition:- whenever we have the info. of distinct values of a columns on which the partitioning is being applied or whenever we want make a partition of a table based on few values of column where partition is being applied we use Static partition.\n",
    "For static partition we need to specify the partition column value in each and every LOAD statement.\n",
    "\n",
    "Dynamic Partition:- whenever we don’t have the idea of the no. of distinct values of the column where the partition is being applied we try to use dynamic partition.\n",
    "For dynamic partition we need to specify the partition column value in each and every LOAD statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddde1d2-f97a-42b5-8dde-b873dc528cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "###23.How do you check if a particular partition exists?\n",
    "Ans:-\n",
    "SHOW PARTITIONS table_name\n",
    "PARTITION(partitioned_column=’partition_value’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ca5955-e465-4bc1-b45d-1d90d98d0ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###24.How can you stop a partition form being queried?\n",
    "By using the ENABLE OFFLINE clause with ALTER TABLE atatement.\n",
    "Syntax:-\n",
    "ALTER TABLE tabele_name PARTITION (PARTITION_SPEC) ENABLE OFFLINE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b36da10a-6488-4a2e-b35a-ac73eb084c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "###25.Why do we need buckets? How Hive distributes the rows into buckets?\n",
    "Ans:- Bucketing in Hive is used to improve performance by eliminating table scans when dealing with a large set of data on a HDFS. Bucketing is a technique to split the data into more manageable files, (By specifying the number of buckets to create). The value of the bucketing column will be hashed by a user-defined number into buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c6bd1-326d-4f08-b6d0-e38b2eff2655",
   "metadata": {},
   "outputs": [],
   "source": [
    "###26.In Hive, how can you enable buckets?\n",
    "Ans:- set.hive.enforce.bucketing=true;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86882df4-95e8-455b-a62f-b178a7ea561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###27.How does bucketing help in the faster execution of queries?\n",
    "Ans:- In bucketing, the partitions can be subdivided into buckets based on the hash function of a column. It gives us a sorted extra structure to the data which can be used for more efficient queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1324c9bf-b815-46b4-b2a2-85650a7a43e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###28.How to optimise Hive Performance? Explain in very detail.\n",
    "a. using Tez-Execution Engine in Hive:- Tez, it is a new application framework built on Hadoop Yarn. That executes complex-DAG of general data processing tasks. However, we can consider it to be a much more flexible and powerful successor to the map-reduce framework. It can work with petabytes of data over thousands of nodes it allows those data access applications.\n",
    "\n",
    "b. By Using the Suitable File Format in Hive:- we can use appropriate file format on the basis of data. It will drastically increase our query performance. or increasing your query performance ORC file format is best suitable because it can can store data in an optimized way than the other file formats. ORC reduces the size of the original data up to 75%.\n",
    "\n",
    "c. Hive Partitioning:- by Partitioning all the entries for the various columns of the dataset are segregated and stored in their respective partition. Hence, While we write the query to fetch the values from the table, only the required partitions of the table are queried. Thus it reduces the time taken by the query to yield the result.\n",
    "\n",
    "d. Hive Bucketing:- Hive offers Bucketing concept. Basically, that allows the user to divide table data sets into more manageable parts. Hence, to maintain parts that are more manageable we can use Bucketing. Through it, the user can set the size of the manageable parts or Buckets too.\n",
    "\n",
    "e. Hive Vectorisation:- Hive Optimization Techniques, to improve the performance of operations we use Vectorized query execution. Here operations refer to scans, aggregations, filters, and joins. It significantly improves query execution time, and is easily enabled with two parameters settings:-\n",
    "set hive.vectorized.execution = true\n",
    "set hive.vectorized.execution.enabled = true\n",
    "\n",
    "f. using Parallel execution at a Mapper & Reducer level:- We can improve the performance of aggregations, filters, and joins of our hive queries by using vectorized query execution, which means scanning them in batches of 1024 rows at once instead of single row each time. We should explore the below parameters which will help to bring in more parallelism and which significantly improves query execution time:\n",
    "set hive.vectorized.execution.enabled=true; \n",
    "set hive.exec.parallel=true;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b170c58-1725-4096-bc2d-5a8ff560443e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###29. What is the use of Hcatalog?\n",
    "Ans:-\n",
    "HCatalog is a tool that allows you to access Hive metastore tables within Pig, Spark SQL, and/or custom MapReduce\n",
    "applications. HCatalog has a REST interface and command line client that allows you to create tables or do other operations. You then write your applications to access the tables using HCatalog libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c17bd28-8243-4847-aee7-60f58e190733",
   "metadata": {},
   "outputs": [],
   "source": [
    "###30. Explain about the different types of join in Hive.\n",
    "Ans:-\n",
    "JOIN --\n",
    "JOIN clause is used to combine and retrieve the records from multiple tables. JOIN is same as OUTER JOIN in SQL. A JOIN\n",
    "condition is to be raised using the primary keys and foreign keys of the tables.\n",
    "\n",
    "LEFT OUTER JOIN-- \n",
    "The HQL LEFT OUTER JOIN returns all the rows from the left table, even if there are no matches in the right table. This \n",
    "means, if the ON clause matches 0 (zero) records in the right table, the JOIN still returns a row in the result, but \n",
    "with NULL in each column from the right table.A LEFT JOIN returns all the values from the left table, plus the matched \n",
    "values from the right table, or NULL in case of no matching JOIN predicate.\n",
    "\n",
    "RIGHT OUTER JOIN --\n",
    "The HQL RIGHT OUTER JOIN returns all the rows from the right table, even if there are no matches in the left table. If \n",
    "the ON clause matches 0 (zero) records in the left table, the JOIN still returns a row in the result, but with NULL in \n",
    "each column from the left table. A RIGHT JOIN returns all the values from the right table, plus the matched values from \n",
    "the left table, or NULL in case of no matching join predicate.\n",
    "\n",
    "FULL OUTER JOIN -- \n",
    "The HiveQL FULL OUTER JOIN combines the records of both the left and the right outer tables that fulfil the JOIN \n",
    "condition. The joined table contains either all the records from both the tables, or fills in NULL values for missing \n",
    "matches on either side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59ff93-b526-415b-825f-c5b2d215f84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###31.Is it possible to create a Cartesian join between 2 tables, using Hive?\n",
    "Ans:- Yes it’s possible.\n",
    "table_reference [CROSS] JOIN table_reference join_condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faa49ba-5536-4d5e-a45e-7beea85bce47",
   "metadata": {},
   "outputs": [],
   "source": [
    "###32.Explain the SMB Join in Hive?\n",
    "Ans:- SMB is a join performed on bucket tables that have the same sorted, bucket, and join condition columns. It reads\n",
    "data from both bucket tables and performs common joins (map and reduce triggered) on the bucket tables. We need to \n",
    "enable the following properties to use SMB:\n",
    "> SET hive.input.format=org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n",
    "> SET hive.auto.convert.sortmerge.join=true;\n",
    "> SET hive.optimize.bucketmapjoin=true;\n",
    "> SET hive.optimize.bucketmapjoin.sortedmerge=true;\n",
    "> SET hive.auto.convert.sortmerge.join.noconditionaltask=true;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eecfd434-d89a-4376-9a06-cf9c58ffb3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###33.What is the difference between order by and sort by which one we should use?\n",
    "Ans --\n",
    "SORT BY clause should be used instead of ORDER BY when one has to sort huge datasets. SORT BY sorts the data using \n",
    "multiple reducers while ORDER BY sorts all data together using a single reducer. Thus, if ORDER BY is used against a \n",
    "large number of inputs, then the execution will be time-consuming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9d0ef-450e-4f21-b855-bd9b50394ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###34.What is the usefulness of the DISTRIBUTED BY clause in Hive?\n",
    "Ans:- Distribute by:-\n",
    " - It distribute the input rows among reducers. It ensures that all rows for the same key columns are going to the same\n",
    "    reducer.\n",
    " - It does not sort the data globally and not locally\n",
    " - No overlapping of data.\n",
    " - DISTRIBUTE BY clause is used to distribute the input rows among reducers. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d7f60-c12c-40c2-9cda-0c6d2da6f156",
   "metadata": {},
   "outputs": [],
   "source": [
    "###35.How does data transfer happen from HDFS to Hive?\n",
    "Ans:- In case of HIVE, there is no transfer of data.\n",
    "HIVE provides you with the functionality to see your flat file data in tabular form and perform some SQL like queries. \n",
    "There are 2 types of tables in HIVE, External and Internal. In both the cases, there is no physical table present. \n",
    "In case of an external table, the HIVE table is mapped to a directory in HDFS. When you put a file in that directory, \n",
    "the data gets reflected in the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c1a10-3bcc-40b5-a3a8-ebebd4b977d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "###36.Wherever (Different Directory) I run the hive query, it creates a new metastore_db, please explain the reason for it?\n",
    "Ans:- This is because we you use Embedded derby mode by default. To use single metastore_db location. you need to change the metastore properties which are available in configuration file hive-site.xml. The property of interest here is javax.jdo.option.ConnectionURL. The default value of this property is jdbc:derby:;databaseName=metastore_db;create=true. This value specifies that you will be using embedded derby as your Hive metastore and the location of the metastore is metastore_db. Also the metastore will be created if it doesn't already exist. So to change the behavior change the location to absolute path, so metastore will be used from that location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c0b26-4a63-490f-a1d7-1a09be40e7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "###37.What will happen in case you have not issued the command: ‘SET hive.enforce.bucketing=true;’ before bucketing a table in Hive?\n",
    "Ans:- the number of files that will be generated in the table directory to be not equal to the number of buckets.\n",
    "The command set hive.enforce.bucketing = true; allows the correct number of reducers and the cluster by column to be \n",
    "automatically selected based on the table. Otherwise, you would need to set the number of reducers to be the same as the \n",
    "number of buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6694c01-bc9c-4cc9-83dd-f23c08b986b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "###38.Can a table be renamed in Hive?\n",
    "Yes, you can change a table name in Hive. You can rename a table name by using: Alter Table table_name RENAME TO new_name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcbd469-3542-45fd-90d2-109df1a5e89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "###39.Write a query to insert a new column(new_col INT) into a hive table at a position before an existing column (x_col)\n",
    "The following query will insert a new column:\n",
    "\n",
    "ALTER TABLE h_table\n",
    "\n",
    "CHANGE COLUMN new_col INT\n",
    "\n",
    "BEFORE x_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a093f9-e9c5-4093-b7b4-d19811f7f806",
   "metadata": {},
   "outputs": [],
   "source": [
    "###40.What is serde operation in HIVE?\n",
    "Ans. Basically, for Serializer/Deserializer, SerDe is an acronym. However, for the purpose of IO, Hive uses the Hive \n",
    "SerDe interface.\n",
    "Hence, it handles both serialization and deserialization in Hive. Also, interprets the results of serialization as individual fields for processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4823e898-1428-4b0c-b714-d7792b58c67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "###41.Explain how Hive Deserializes and serialises the data?\n",
    "Serialization — Process of converting an object in memory into bytes that can be stored in a file or transmitted over a network.\n",
    "Deserialization — Process of converting the bytes back into an object in memory. Java understands objects and hence object is a deserialized state of data. When you use the same concept, Hive understands “columns” and hence if given a “row” of data, the task of converting that data into columns is the Deserialization part of Hive SerDe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509a27c2-5834-4bbc-8a81-cc71a97d66fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "###42.Write the name of the built-in serde in hive.\n",
    "Ans:- The Hive SerDe library is in org.apache.hadoop.hive.serde2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c6d659-b44a-4655-b127-45fdb38ff4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "###43.What is the need of custom Serde?\n",
    "Ans:- Sometimes data is need not a proper format which build-in serde can handle for IO operation. There is need of \n",
    "custom SerDe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202ee7d3-401f-4214-b8ab-f4e7fcf32cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "###44.Can you write the name of a complex data type(collection data types) in Hive?\n",
    "Ans:- Struct, MAP and Array ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e7cc72-9bc5-4183-b580-fad1a1290e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "###45.Can hive queries be executed from script files? How?\n",
    "Ans:- Yes, we can execute hive quries from script files.\n",
    "source /path/to/file/file_with_query.hql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cf2a68-2435-4103-8571-721f0c4a404f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###46.What are the default record and field delimiter used for hive text files?\n",
    "Ans:- \n",
    "The default record delimiter is − \\n\n",
    "The filed delimiters are − \\001,\\002,\\003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a04d99d-c9fd-4ccc-81e0-07107d187463",
   "metadata": {},
   "outputs": [],
   "source": [
    "###47.How do you list all databases in Hive whose name starts with s?\n",
    "Ans:- SHOW DATABASES LIKE 's%';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce3b1c91-7dc5-4630-ae31-5b5de82f2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###48.What is the difference between LIKE and RLIKE operators in Hive?\n",
    "Ans:- \n",
    "LIKE -- We use LIKE to search for string with similar text.\n",
    "RLIKE -- It is used to search the advanced Regular expression pattern on the columns. If the given pattern matches with any substring of the column, the function returns TRUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d8b40e-8afa-4177-9092-28c1433892f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "###49.How to change the column data type in Hive?\n",
    "Ans:- ALTER TABLE table_name CHANGE column_name column_name new_datatype;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd6dfdc-83a6-4b80-8c66-88094e6e251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "###50.How will you convert the string ’51.2’ to a float value in the particular column?\n",
    "Ans:- select cast( ’51.2’ as float);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f332677-a610-4015-86c0-e2954ac69a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###51.What will be the result when you cast ‘abc’ (string) as INT?\n",
    "Ans:- Null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0089ff23-c704-4e4b-ae27-3b32d253bc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "###52.What does the following query do?\n",
    "a. INSERT OVERWRITE TABLE employees\n",
    "b. PARTITION (country, state)\n",
    "c. SELECT ..., se.cnty, se.st\n",
    "d. FROM staged_employees se;\n",
    "Ans:- It inserts the values or if something is available already it overwrites values in table employees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b1ce40-dbe9-429b-a06a-b5d7fcb67964",
   "metadata": {},
   "outputs": [],
   "source": [
    "###53.Write a query where you can overwrite data in a new table from the existing table ?\n",
    "Ans:- CREATE TABLE NEW_TABLE AS SELECT col_1, col_2 FROM OLD_TABLE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a5bf83-da6a-4496-acf3-bf050fee578f",
   "metadata": {},
   "outputs": [],
   "source": [
    "###54.What is the maximum size of a string data type supported by Hive? Explain how Hive supports binary formats.\n",
    "Ans:-By default, the columns metadata for Hive does not specify a maximum data length for STRING columns.The driver has\n",
    "the parameter DefaultStringColumnLength, default is 255 maximum value. Hive supports two more primitive data types, \n",
    "BOOLEAN and BINARY.BINARY is an array of Bytes. BINARY columns are stored within the record, not separately like BLOBs.\n",
    "We can include arbitrary bytes in BINARY column and these bytes are not parsed by Hive as numbers or strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b801bc-07b8-471e-9c60-1cc514f32b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "###55. What File Formats and Applications Does Hive Support?\n",
    "Ans:- Sequence files, ORC files, Avro data files, and Parquet file formats etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef0663-dcca-4bc5-b0fd-a15eec185353",
   "metadata": {},
   "outputs": [],
   "source": [
    "###56.How do ORC format tables help Hive to enhance its performance?\n",
    "Ans:- ORC file format is best suitable because it can store data in an optimized way than the other file formats. ORC\n",
    "reduces the size of the original data up to 75%. This reduced file size increases the query performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bdc227-8c34-496b-8eeb-98e9126ce707",
   "metadata": {},
   "outputs": [],
   "source": [
    "###57.How can Hive avoid mapreduce while processing the query?\n",
    "Ans:-\n",
    "hive.fetch.task.conversion property can (FETCH task) minimize latency of mapreduce overhead. When queried SELECT, FILTER, LIMIT queries, this property skip mapreduce and using FETCH task. As a result Hive can execute query without run mapreduce task.\n",
    "\n",
    "\n",
    "###58.What is view and indexing in hive?\n",
    "Ans:-\n",
    "View:-\n",
    "- Basically, Apache Hive View is similar to Hive tables, that are generated on the basis of requirements.\n",
    "- Apache Hive View is a searchable object in a database which we can define by the query. However, we can not store data in the view. Still, some refer to as a view as “virtual tables”.\n",
    "- Hence, we can query a view like we can a table. Moreover, by using joins it is possible to combine data from or more table. Also, it contains a subset of information.\n",
    "\n",
    "Indexing in hive:-\n",
    "- On defining indexing in Hive we can say these are pointers to particular column name of a table\n",
    "- However, the user has to manually define the Hive index\n",
    "- Basically, we are creating the pointer to particular column name of the table, wherever we are creating Hive index.\n",
    "- By using the Hive index value created on the column name, any Changes made to the column present in tables are stored.\n",
    "\n",
    "\n",
    "###59.Can the name of a view be the same as the name of a hive table?\n",
    "Ans:- No, use different names for a view and it’s table.\n",
    "\n",
    "\n",
    "###60.What types of costs are associated in creating indexes on hive tables?\n",
    "Ans:- A processing cost in arranging the values of the column on which index is created since Indexes occupies.\n",
    "\n",
    "\n",
    "###61.Give the command to see the indexes on a table.\n",
    "Ans:- SHOW INDEX ON table_name;\n",
    "\n",
    "\n",
    "###62. Explain the process to access subdirectories recursively in Hive queries.\n",
    "Ans:- We can do this with the help of below commands -\n",
    "hive> Set mapred.input.dir.recursive=true;\n",
    "hive> Set hive.mapred.supports.subdirectories=true;\n",
    "\n",
    "\n",
    "###63.If you run a select * query in Hive, why doesn't it run MapReduce?\n",
    "Ans:- Hive fetches the whole data from file as a FetchTask rather than a mapreduce task which just dumps the data as it is without doing anything on it.\n",
    "\n",
    "\n",
    "###64.What are the uses of Hive Explode?\n",
    "Ans:- Explode is a User Defined Table generating Function(UDTF) in Hive. It takes an array (or a map) as an input and outputs the elements of the array (or a map) as separate rows.\n",
    "\n",
    "\n",
    "###65.What is the available mechanism for connecting applications when we\n",
    "run Hive as a server?\n",
    "Ans:-\n",
    " - Thrift Client\n",
    " - ODBC Driver\n",
    " - JDBC Driver\n",
    "\n",
    "\n",
    "###66.Can the default location of a managed table be changed in Hive?\n",
    "Ans:- Yes. Just define the custom location with the location clause while creating a table\n",
    "\n",
    "\n",
    "###67.What is the Hive ObjectInspector function?\n",
    "Ans:- it is used to analyze the internal structure of the row object and also the structure of the individual columns. \n",
    "ObjectInspector provides a uniform way to access complex objects that can be stored in multiple formats in the memory.\n",
    "\n",
    "###68.What is UDF in Hive?\n",
    "Ans:-\n",
    "UDF stands for User Defined Functions. Hive is a powerful tool that allows us to provision sql queries on top of stored data for basic querying and/or analysis, and on top of an already rich set of built-in functions, it allows us to extend its functionality by writing custom functions of our own.\n",
    "\n",
    "\n",
    "###69.Write a query to extract data from hdfs to hive.\n",
    "Ans:- load data inpath '<hdfs location>' into table <hive table name>\n",
    "\n",
    "\n",
    "###70.What is TextInputFormat and SequenceFileInputFormat in hive.\n",
    "Ans:-\n",
    "TextInputFormat -- It is the default InputFormat. This InputFormat treats each line of each input file as a separate record. It performs no parsing. TextInputFormat is useful for unformatted data or line-based records like log files. Hence, \n",
    "Key – It is the byte offset of the beginning of the line within the file (not whole file one split). So it will be unique if combined with the file name.\n",
    "Value – It is the contents of the line. It excludes line terminators.\n",
    "\n",
    "SequenceFileInputFormat --\n",
    "It is an InputFormat which reads sequence files. Sequence files are binary files. These files also store sequences of binary key-value pairs. These are block-compressed and provide direct serialization and deserialization of several arbitrary data. Hence, Key & Value both are user-defined.\n",
    "\n",
    "\n",
    "###71.How can you prevent a large job from running for a long time in a hive?\n",
    "Ans:- by setting the MapReduce jobs to execute in strict mode\n",
    "set hive.mapred.mode=strict;\n",
    "\n",
    "\n",
    "###72.When do we use explode in Hive?\n",
    "Ans:- Explode is a User Defined Table generating Function(UDTF) in Hive. It takes an array (or a map) as an input and outputs the elements of the array (or a map) as separate rows.\n",
    "\n",
    "\n",
    "###73.Can Hive process any type of data formats? Why? Explain in very detail?\n",
    "Ans:- No, hive can only process the structured data, because the database used by hive by default is Derby (RDBMS).\n",
    "\n",
    "\n",
    "###74.Whenever we run a Hive query, a new metastore_db is created. Why?\n",
    "Ans:- This is because we you use Embedded derby mode by default. To use single metastore_db location. you need to change the metastore properties which are available in configuration file hive-site.xml. The property of interest here is javax.jdo.option.ConnectionURL. The default value of this property is jdbc:derby:;databaseName=metastore_db;create=true. This value specifies that you will be using embedded derby as your Hive metastore and the location of the metastore is metastore_db. Also the metastore will be created if it doesn't already exist. So to change the behavior change the location to absolute path, so metastore will be used from that location.\n",
    "\n",
    "\n",
    "###75.Can we change the data type of a column in a hive table? Write a complete query.\n",
    "Ans:- ALTER TABLE table_name CHANGE column_name column_name new_datatype;\n",
    "\n",
    "\n",
    "###76.While loading data into a hive table using the LOAD DATA clause, howdo you specify it is a hdfs file and not a local file ?\n",
    "Ans:- LOAD DATA INPATH '<hdfs location>' INSERT INTO TABLE ;\n",
    "\n",
    "\n",
    "###77.What is the precedence order in Hive configuration?\n",
    "Ans:- In Hive we can use following precedence order to set the configurable properties.\n",
    "1. SET Command in HIVE\n",
    "2. The command line –hiveconf option\n",
    "3. Hive-site.XML\n",
    "4. Hive-default.xml\n",
    "5. Hadoop-site.xml\n",
    "6. Hadoop-default.xml\n",
    "\n",
    "\n",
    "###78.Which interface is used for accessing the Hive metastore?\n",
    "Ans:-  WebHCat API web interface can be used for Hive commands. It is a REST API that allows applications to make HTTP requests to access the Hive metastore (HCatalog DDL). \n",
    "\n",
    "\n",
    "###79.Is it possible to compress json in the Hive external table ?\n",
    "Ans:- yes, we can gzip our data files and then put them into the hdfs location in the *.gz format\n",
    "\n",
    "\n",
    "###80.What is the difference between local and remote metastores?\n",
    "Ans:-\n",
    "Local Meta-store: In this, the meta-store service runs in the same JVM in which the Hive service runs. It associates with a database running in a different JVM, either on a similar machine or a remote machine.\n",
    "Remote Meta-store: In this, the meta-store service runs alone separating JVM and not in the Hive benefit JVM. Thus, different procedures communicate with the metastore server utilizing Thrift Network APIs. Having at least one meta-store server for this situation will provide greater accessibility.\n",
    "\n",
    "\n",
    "###81.What is the purpose of archiving tables in Hive?\n",
    "Ans:- As we know that the table data in Hadoop is stored in hdfs, and due to the architecture of Hadoop, the number of files in the filesystem directly affects the memory consumption in the namenode. memory usage may hit the limits of accessible memory on a single machine when there are >50-100 million files. In such situations, it is advantageous to have as few files as possible. The use of Hadoop Archives is one approach to reducing the number of files in partitions. Hive has built-in support to convert files in existing partitions to a Hadoop Archive (HAR) so that a partition that may once have consisted of 100's of files can occupy just ~3 files (depending on settings). However, the trade-off is that queries may be slower due to the additional overhead in reading from the HAR.\n",
    "\n",
    "\n",
    "###82.What is DBPROPERTY in Hive?\n",
    "Ans:-  DBPROPERTIES takes multiple arguments in the form of a key-value pair.\n",
    "ALTER (DATABASE|SCHEMA) <database_name> SET DBPROPERTIES ('<property_name>'='<property_value>');\n",
    "\n",
    "\n",
    "###83.Differentiate between local mode and MapReduce mode in Hive.\n",
    "Ans:-\n",
    "MapReduce Mode\n",
    "• Hive script is executed on Hadoop cluster.\n",
    "• The Hive scripts are converted into MapReduce jobs and then executed on Hadoop cluster (hdfs)\n",
    "Local Mode\n",
    "• Hive script runs on a Single machine without the need of Hadoop cluster or hdfs.\n",
    "• Local mode is used for development purpose to see how the script would behave in an actual environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
